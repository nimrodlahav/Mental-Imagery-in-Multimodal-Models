
\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Theoretical Background: Transformers}
\author{Your Name}
\date{\today}

\begin{document}
\maketitle

\section{Transformers}
A Transformer layer consists of \textbf{multi-head self-attention} followed by a \textbf{feedforward network}.
Given hidden states $X \in \mathbb{R}^{n \times d}$:

\subsection{Attention}
Queries, Keys, and Values are computed as:
\begin{equation}
Q = XW^Q, \quad K = XW^K, \quad V = XW^V
\end{equation}

Attention is then:
\begin{equation}
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

For multi-head attention:
\begin{equation}
\text{MHA}(X) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
\end{equation}

\subsection{Feedforward Layer}
Each token embedding is passed through a 2-layer MLP with nonlinearity:
\begin{equation}
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
\end{equation}

\end{document}
