# Mental Imagery in Multimodal Models

This code collects feed-forward activations from a LLaVA (vision-language) model on COCO images and saves them to Parquet for analysis.

## Setup
```bash
pip install -r requirements.txt
