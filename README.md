# Mental Imagery in Multimodal Models

Collect feed-forward activations from a LLaVA (vision-language) model on COCO images and save them to Parquet for analysis.

## Setup
```bash
pip install -r requirements.txt
